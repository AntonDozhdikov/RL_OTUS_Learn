{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOAflic27cHmmcaxx3/w2zf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonDozhdikov/RL_OTUS_Learn/blob/main/Car_racing_v3_A2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cjx1OS-7_go8"
      },
      "outputs": [],
      "source": [
        "# @title установка драйверов\n",
        "# Команда apt-get update обновляет индекс пакетов, чтобы получить последнюю информацию о доступных версиях пакетов.\n",
        "!apt-get update > /dev/null 2>&1\n",
        "\n",
        "# Установка необходимых пакетов: xvfb (виртуальный фреймбуфер X), Python OpenGL и ffmpeg (для обработки видео).\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# Установка пакетов для разработки на C/C++ и Python: swig (интерфейс между языками программирования), build-essential (инструменты сборки), python-dev и python3-dev (заголовочные файлы и библиотеки для Python).\n",
        "!apt-get install -y swig build-essential python-dev python3-dev > /dev/null 2>&1\n",
        "\n",
        "# Установка утилит для работы с X-сервером, таких как xrandr и xdpyinfo.\n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "\n",
        "# Установка виртуального фреймбуфера X (xvfb), который позволяет запускать графические приложения без физического дисплея.\n",
        "!apt-get install xvfb > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title установка библиотек\n",
        "#%%capture\n",
        "# Устанавливает библиотеку rarfile для работы с архивами RAR.\n",
        "!pip install rarfile --quiet\n",
        "\n",
        "# Устанавливает пакет stable-baselines3 с дополнительными зависимостями.\n",
        "!pip install stable-baselines3[extra] --quiet\n",
        "\n",
        "# Устанавливает библиотеку Ale-Py для работы с Atari играми.\n",
        "!pip install ale-py --quiet\n",
        "\n",
        "# Устанавливает пакет gym с поддержкой Box2D для физических симуляций.\n",
        "!pip install gym[box2d] --quiet\n",
        "\n",
        "# Устанавливает PyVirtualDisplay для создания виртуальных дисплеев. Страшно кривая вещь!!! Ни в колабе ни на машине не работает\n",
        "!pip install pyvirtualdisplay --quiet\n",
        "\n",
        "# Устанавливает Pyglet для работы с графикой и мультимедиа.\n",
        "!pip install pyglet --quiet\n",
        "\n",
        "# Устанавливает PyGame для разработки игр.\n",
        "!pip install pygame --quick\n",
        "\n",
        "# Устанавливает Minigrid для создания простых gridworld сред.\n",
        "!pip install minigrid --quiet\n",
        "\n",
        "# Устанавливает SWIG для взаимодействия между различными языками программирования.\n",
        "!pip install -q swig --quiet\n",
        "\n",
        "# Устанавливает Gymnasium с поддержкой Box2D для физических симуляций.\n",
        "!pip install -q gymnasium[box2d] --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r52g1aiK_5Bw",
        "outputId": "a9d04caf-36e9-4d21-ac80-39c7fd83a9ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --quick\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.7/136.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт необходимых библиотек\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.normal import Normal\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import os\n",
        "import warnings"
      ],
      "metadata": {
        "id": "SaZ9ganY_5JP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Фильтрация предупреждений\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# Гиперпараметры\n",
        "ENV_NAME = \"CarRacing-v3\"       # Установка верссии среды\n",
        "LEARNING_RATE_ACTOR = 0.0001    # Скорость обучения для нейронной сети актёра (Actor Network)\n",
        "LEARNING_RATE_CRITIC = 0.0002   # Скорость обучения для нейронной сети критика (Critic Network)\n",
        "GAMMA = 0.99                    # Коэффициент дисконтирования будущих вознаграждений\n",
        "EPISODES = 300                  # Общее количество эпизодов для тренировки агента\n",
        "BATCH_SIZE = 64                 # Размер мини-пакета данных для обновления весов модели\n",
        "SAVE_INTERVAL = 30              # Интервал шагов для сохранения модели\n",
        "HIDDEN_SIZE = 256               # Размер скрытого слоя в нейронных сетях\n",
        "LOG_INTERVAL = 10               # Интервал шагов для логгирования результатов\n",
        "IMAGE_SIZE = (96, 96)           # Размер входного изображения для обработки моделью"
      ],
      "metadata": {
        "id": "iAWd0LGo_5MB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Папки для сохранения\n",
        "MODEL_DIR = \"saved_models\"      # Директория для сохранения моделей\n",
        "LOG_DIR = \"logs\"                # Директория для сохранения логов\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)     # Создаем директорию MODEL_DIR, если она еще не существует\n",
        "os.makedirs(LOG_DIR, exist_ok=True)       # Создаем директорию LOG_DIR, если она еще не существует"
      ],
      "metadata": {
        "id": "5tEx4ZA-_5OT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Предобработка изображения\n",
        "def preprocess_image(image):\n",
        "    # Конвертация в оттенки серого и изменение размера\n",
        "    image = image.mean(axis=2)  # Усреднение по RGB каналам\n",
        "    image = image[::2, ::2]     # Уменьшение разрешения до 48x48\n",
        "    image = image / 255.0       # Нормализация [0, 1]\n",
        "    return image[np.newaxis, ...]  # Добавление размерности канала"
      ],
      "metadata": {
        "id": "4ibw7kZj_5Qu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Архитектура сети актора\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, action_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 8, stride=4),          # Первый сверточный слой с 32 фильтрами размером 8x8 и шагом 4\n",
        "            nn.ReLU(),                              # Функция активации ReLU\n",
        "            nn.Conv2d(32, 64, 4, stride=2),         # Второй сверточный слой с 64 фильтрами размером 4x4 и шагом 2\n",
        "            nn.ReLU(),                              # Функция активации ReLU\n",
        "            nn.Conv2d(64, 64, 3, stride=1),         # Третий сверточный слой с 64 фильтрами размером 3x3 и шагом 1\n",
        "            nn.ReLU(),                              # Функция активации ReLU\n",
        "            nn.Flatten()                            # Слой для преобразования многомерного тензора в одномерный\n",
        "        )\n",
        "        conv_out_size = self._get_conv_out_size((1, 48, 48))  # Определение размера выходного сигнала сверток\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, HIDDEN_SIZE),  # Полносвязный слой с числом нейронов равным значению HIDDEN_SIZE\n",
        "            nn.ReLU(),                              # Функция активации ReLU\n",
        "            nn.Linear(HIDDEN_SIZE, action_dim),     # Выходной полносвязный слой с количеством нейронов равным числу возможных действий\n",
        "            nn.Tanh()                               # Функция активации Tanh для ограничения выходных значений в диапазоне [-1, 1]\n",
        "        )\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))  # Логарифм стандартного отклонения для нормального распределения\n",
        "\n",
        "    def _get_conv_out_size(self, shape):\n",
        "        dummy = torch.zeros(1, *shape)                   # Создание нулевого тензора для определения размера выхода сверток\n",
        "        return self.conv(dummy).view(1, -1).size(1)      # Возвращает размерность выходного сигнала сверток\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)                                 # Пропускаем данные через сверточную часть сети\n",
        "        mu = self.fc(x)                                  # Получаем среднее значение для нормального распределения\n",
        "        std = torch.exp(self.log_std)                    # Вычисляем стандартное отклонение\n",
        "        return mu, std                                   # Возвращаем среднее и стандартное отклонение\n"
      ],
      "metadata": {
        "id": "sP-AuvhC_5Tj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Архитектура сети критика\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()                  # Вызываем конструктор родительского класса\n",
        "        self.conv = nn.Sequential(                       # Последовательность слоев свертки\n",
        "            nn.Conv2d(1, 32, 8, stride=4),               # Первый сверточный слой с 32 фильтрами размером 8x8 и шагом 4\n",
        "            nn.ReLU(),                                   # Функция активации ReLU\n",
        "            nn.Conv2d(32, 64, 4, stride=2),              # Второй сверточный слой с 64 фильтрами размером 4x4 и шагом 2\n",
        "            nn.ReLU(),                                   # Функция активации ReLU\n",
        "            nn.Conv2d(64, 64, 3, stride=1),              # Третий сверточный слой с 64 фильтрами размером 3x3 и шагом 1\n",
        "            nn.ReLU(),                                   # Функция активации ReLU\n",
        "            nn.Flatten()                                 # Слой для преобразования многомерного тензора в одномерный\n",
        "        )\n",
        "        conv_out_size = self._get_conv_out_size((1, 48, 48))  # Определение размера выходного сигнала сверток\n",
        "        self.fc = nn.Sequential(                          # Последовательность полносвязных слоев\n",
        "            nn.Linear(conv_out_size, HIDDEN_SIZE),        # Полносвязный слой с числом нейронов равным значению HIDDEN_SIZE\n",
        "            nn.ReLU(),                                    # Функция активации ReLU\n",
        "            nn.Linear(HIDDEN_SIZE, 1)                     # Выходной полносвязный слой с одним нейроном для оценки значения состояния\n",
        "        )\n",
        "\n",
        "    def _get_conv_out_size(self, shape):\n",
        "        dummy = torch.zeros(1, *shape)                    # Создание нулевого тензора для определения размера выхода сверток\n",
        "        return self.conv(dummy).view(1, -1).size(1)       # Возвращает размерность выходного сигнала сверток\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)                                  # Пропускаем данные через сверточную часть сети\n",
        "        return self.fc(x)                                 # Пропускаем результат через полносвязные слои и возвращаем оценку значения состояния\n"
      ],
      "metadata": {
        "id": "MlUNhAlJ_5cQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Агент A2C\n",
        "class A2CAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env                                # Сохраняем среду окружения\n",
        "        self.action_dim = env.action_space.shape[0]   # Определяем размерность пространства действий\n",
        "\n",
        "        # Инициализация сетей\n",
        "        self.actor = Actor(self.action_dim)           # Создаем экземпляр сети актора\n",
        "        self.critic = Critic()                        # Создаем экземпляр сети критика\n",
        "\n",
        "        # Оптимизаторы\n",
        "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=LEARNING_RATE_ACTOR)  # Оптимизатор для актора\n",
        "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=LEARNING_RATE_CRITIC)  # Оптимизатор для критика\n",
        "\n",
        "        # Буферы\n",
        "        self.states = []                      # Список состояний\n",
        "        self.actions = []                     # Список действий\n",
        "        self.rewards = []                     # Список наград\n",
        "        self.dones = []                       # Список индикаторов завершения эпизода\n",
        "        self.values = []                      # Список оценочных значений состояний\n",
        "\n",
        "        # Логирование\n",
        "        self.log = {'episode': [], 'reward': [], 'length': []}  # Словарь для хранения информации об эпизодах\n",
        "        self.best_reward = -np.inf            # Лучшая награда, изначально равна минус бесконечности\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Преобразуем состояние в тензор PyTorch\n",
        "        with torch.no_grad():  # Отключаем вычисление градиентов для выбора действия\n",
        "            mu, std = self.actor(state_tensor)  # Получаем среднее и стандартное отклонение из сети актёра\n",
        "            dist = Normal(mu, std)              # Создаем нормальное распределение\n",
        "            action = dist.sample()              # Генерируем случайное действие из распределения\n",
        "        return action.numpy()[0], dist, self.critic(state_tensor).item()  # Возвращаем выбранное действие, распределение и оценку состояния\n",
        "    def update(self):\n",
        "        states = torch.FloatTensor(np.array(self.states))  # Преобразуем список состояний в тензор PyTorch\n",
        "        actions = torch.FloatTensor(np.array(self.actions))  # Преобразуем список действий в тензор PyTorch\n",
        "        rewards = torch.FloatTensor(np.array(self.rewards))  # Преобразуем список наград в тензор PyTorch\n",
        "        dones = torch.FloatTensor(np.array(self.dones))  # Преобразуем список индикаторов завершения в тензор PyTorch\n",
        "\n",
        "        # Пересчитываем значения через критика с включенными градиентами\n",
        "        current_values = self.critic(states).squeeze()  # Оценка текущих состояний сетью критика\n",
        "\n",
        "        # Расчёт возвратов с дисконтированием\n",
        "        returns = []  # Список для накопления возвратов\n",
        "        R = 0  # Начальное значение возврата\n",
        "        for r, done in zip(reversed(rewards.numpy()), reversed(dones.numpy())):  # Проходим назад по спискам наград и завершений\n",
        "            R = r + GAMMA * R * (1 - done)  # Рассчитываем возврат с учётом коэффициента дисконтирования\n",
        "            returns.insert(0, R)  # Добавляем рассчитанный возврат в начало списка\n",
        "        returns = torch.FloatTensor(returns)  # Преобразуем список возвратов в тензор PyTorch\n",
        "\n",
        "        # Нормализация преимуществ\n",
        "        advantages = returns - current_values  # Разница между возвратами и текущими значениями (преимущества)\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)  # Нормализуем преимущества\n",
        "\n",
        "        # Обновление критика\n",
        "        self.optimizer_critic.zero_grad()  # Сбрасываем накопленные градиенты оптимизатора критика\n",
        "        value_loss = F.mse_loss(current_values, returns)  # Рассчитываем ошибку как среднеквадратичное отклонение между текущими значениями и возвратами\n",
        "        value_loss.backward()  # Выполняем обратное распространение ошибки\n",
        "        self.optimizer_critic.step()  # Применяем обновлённые веса к сети критика\n",
        "\n",
        "        # Обновление актёра\n",
        "        self.optimizer_actor.zero_grad()  # Сбрасываем накопленные градиенты оптимизатора актора\n",
        "        mu, std = self.actor(states)  # Получаем среднее и стандартное отклонение из сети актора\n",
        "        dist = Normal(mu, std)  # Создаём нормальное распределение\n",
        "        log_probs = dist.log_prob(actions).sum(-1)  # Рассчитываем логарифмические вероятности действий\n",
        "        actor_loss = -(log_probs * advantages.detach()).mean()  # Рассчитываем потерю актора, умножая логарифмы вероятностей на нормализованные преимущества\n",
        "        actor_loss.backward()  # Выполняем обратное распространение ошибки\n",
        "        self.optimizer_actor.step()  # Применяем обновлённые веса к сети актора\n",
        "\n",
        "        self._clear_buffers()  # Очищаем буферы для следующего шага\n",
        "    def _clear_buffers(self):\n",
        "        self.states.clear()       # Очистить список состояний\n",
        "        self.actions.clear()      # Очистить список действий\n",
        "        self.rewards.clear()      # Очистить список наград\n",
        "        self.dones.clear()        # Очистить список индикаторов завершения\n",
        "        self.values.clear()       # Очистить список оценочных значений\n",
        "\n",
        "    def save_model(self, episode):\n",
        "        torch.save({                                      # Сохраняем модель и сопутствующие данные\n",
        "            'actor': self.actor.state_dict(),             # Сохраняем состояние сети актора\n",
        "            'critic': self.critic.state_dict(),           # Сохраняем состояние сети критика\n",
        "            'optimizer_actor': self.optimizer_actor.state_dict(),  # Сохраняем состояние оптимизатора актора\n",
        "            'optimizer_critic': self.optimizer_critic.state_dict(),  # Сохраняем состояние оптимизатора критика\n",
        "            'episode': episode                           # Сохраняем номер текущего эпизода\n",
        "        }, f'{MODEL_DIR}/a2c_{ENV_NAME}_ep_{episode}.pth')  # Имя файла для сохранения модели\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        checkpoint = torch.load(model_path)  # Загружаем сохранённую модель\n",
        "        self.actor.load_state_dict(checkpoint['actor'])  # Восстанавливаем состояние сети актора\n",
        "        self.critic.load_state_dict(checkpoint['critic'])  # Восстанавливаем состояние сети критика\n",
        "        self.optimizer_actor.load_state_dict(checkpoint['optimizer_actor'])  # Восстанавливаем состояние оптимизатора актора\n",
        "        self.optimizer_critic.load_state_dict(checkpoint['optimizer_critic'])  # Восстанавливаем состояние оптимизатора критика\n",
        "        return checkpoint['episode']  # Возвращаем номер последнего эпизода\n",
        "\n",
        "    def plot_training(self):\n",
        "        plt.figure(figsize=(10,5))  # Создаём фигуру для графика\n",
        "        plt.plot(self.log['episode'], self.log['reward'], label='Средняя награда')  # Строим график средней награды по эпизодам\n",
        "        plt.plot(self.log['episode'],  # Строим график скользящего среднего\n",
        "                 np.convolve(self.log['reward'], np.ones(100)/100, mode='same'),  # Скользящее среднее с окном 100\n",
        "                 label='Скользящее среднее (100 эп)')\n",
        "        plt.xlabel('Эпизоды')  # Подписываем ось X\n",
        "        plt.ylabel('Суммарная награда')  # Подписываем ось Y\n",
        "        plt.title('Динамика обучения')  # Указываем заголовок графика\n",
        "        plt.legend()  # Показать легенду\n",
        "        plt.grid()  # Включаем сетку\n",
        "        plt.savefig(f'{LOG_DIR}/training_curve.png')  # Сохраняем график в файл\n",
        "        plt.close()  # Закрываем текущий график\n"
      ],
      "metadata": {
        "id": "jKEkuZvMARHk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение агента\n",
        "def train():\n",
        "    env = gym.make(ENV_NAME, continuous=True)  # Создаем среду для обучения\n",
        "    agent = A2CAgent(env)  # Инициализируем агента\n",
        "\n",
        "    print(\"Старт обучения...\")  # Сообщаем о начале обучения\n",
        "    start_time = datetime.now()  # Запоминаем время начала обучения\n",
        "\n",
        "    for ep in range(1, EPISODES+1):  # Основной цикл обучения по эпизодам\n",
        "        state, _ = env.reset()  # Сбрасываем среду и получаем начальное состояние\n",
        "        state = preprocess_image(state)  # Предварительная обработка изображения\n",
        "        episode_reward = 0  # Переменная для подсчета общей награды за эпизод\n",
        "        done = False  # Флаг завершения эпизода\n",
        "        step = 0  # Счетчик шагов внутри эпизода\n",
        "\n",
        "        while not done:  # Цикл выполнения одного эпизода\n",
        "            # Выбор действия\n",
        "            action, dist, value = agent.get_action(state)  # Получаем действие, распределение и оценку состояния\n",
        "\n",
        "            # Шаг в среде\n",
        "            next_state, reward, done, truncated, info = env.step(action)  # Делаем шаг в среде\n",
        "            next_state = preprocess_image(next_state)  # Предварительная обработка следующего состояния\n",
        "\n",
        "            # Обновление буферов\n",
        "            agent.states.append(state)  # Добавляем текущее состояние в буфер\n",
        "            agent.actions.append(action)  # Добавляем выполненное действие в буфер\n",
        "            agent.rewards.append(reward)  # Добавляем полученную награду в буфер\n",
        "            agent.dones.append(done)  # Добавляем индикатор завершения в буфер\n",
        "            agent.values.append(value)  # Добавляем оценку состояния в буфер\n",
        "\n",
        "            state = next_state  # Обновляем текущее состояние\n",
        "            episode_reward += reward  # Накапливаем общую награду за эпизод\n",
        "            step += 1  # Увеличиваем счетчик шагов\n",
        "\n",
        "            # Обновление сетей\n",
        "            if step % BATCH_SIZE == 0 or done:  # Если достигли конца мини-пакета или эпизода\n",
        "                agent.update()  # Обновляем параметры сети\n",
        "\n",
        "            if done or truncated:  # Если эпизод завершился или был прерван\n",
        "                break  # Завершаем выполнение текущего эпизода\n",
        "\n",
        "        # Логирование\n",
        "        agent.log['episode'].append(ep)  # Добавляем номер эпизода в журнал\n",
        "        agent.log['reward'].append(episode_reward)  # Добавляем суммарную награду за эпизод в журнал\n",
        "        agent.log['length'].append(step)  # Добавляем длину эпизода в журнал\n",
        "\n",
        "        # Сохранение лучшей модели\n",
        "        if episode_reward > agent.best_reward:  # Если текущая награда лучше предыдущей\n",
        "            agent.best_reward = episode_reward  # Обновляем лучшую награду\n",
        "            agent.save_model(ep)  # Сохраняем модель\n",
        "            print(f\"Эпизод {ep}: Новая лучшая награда {episode_reward:.2f}\")  # Сообщаем о новой лучшей награде\n",
        "\n",
        "        # Вывод статистики\n",
        "        if ep % LOG_INTERVAL == 0:  # Каждые LOG_INTERVAL эпизодов выводим статистику\n",
        "            avg_reward = np.mean(agent.log['reward'][-LOG_INTERVAL:])  # Рассчитываем среднюю награду за последние LOG_INTERVAL эпизодов\n",
        "            print(f\"Эпизод {ep}, Средняя награда: {avg_reward:.2f}, Время: {datetime.now() - start_time}\")  # Выводим информацию\n",
        "\n",
        "    # Сохранение финальной модели и логирование\n",
        "    agent.save_model(EPISODES)  # Сохраняем последнюю модель\n",
        "    agent.plot_training()  # Строим график обучения\n",
        "    env.close()  # Закрываем среду\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()  # Запускаем обучение\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlBgMbuKARQH",
        "outputId": "356c0126-67c4-4bf9-82dd-5f4b7b3fcaa2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Старт обучения...\n",
            "Эпизод 1: Новая лучшая награда -51.72\n",
            "Эпизод 6: Новая лучшая награда -45.95\n",
            "Эпизод 7: Новая лучшая награда -43.61\n",
            "Эпизод 10: Новая лучшая награда -35.11\n",
            "Эпизод 10, Средняя награда: -49.31, Время: 0:02:45.870070\n",
            "Эпизод 12: Новая лучшая награда -32.66\n",
            "Эпизод 15: Новая лучшая награда -7.75\n",
            "Эпизод 18: Новая лучшая награда 4.65\n",
            "Эпизод 20, Средняя награда: -43.55, Время: 0:05:32.249379\n",
            "Эпизод 30, Средняя награда: -51.68, Время: 0:08:22.894400\n",
            "Эпизод 40, Средняя награда: -76.02, Время: 0:11:00.600902\n",
            "Эпизод 50, Средняя награда: -77.61, Время: 0:13:54.036092\n",
            "Эпизод 60, Средняя награда: -75.93, Время: 0:16:49.259940\n",
            "Эпизод 70, Средняя награда: -73.40, Время: 0:19:37.170477\n",
            "Эпизод 80, Средняя награда: -69.90, Время: 0:22:23.550247\n",
            "Эпизод 90, Средняя награда: -61.67, Время: 0:25:14.911917\n",
            "Эпизод 100, Средняя награда: -73.87, Время: 0:28:03.474330\n",
            "Эпизод 110, Средняя награда: -70.54, Время: 0:30:58.795128\n",
            "Эпизод 120, Средняя награда: -73.49, Время: 0:33:52.973238\n",
            "Эпизод 130, Средняя награда: -65.18, Время: 0:36:31.646023\n",
            "Эпизод 140, Средняя награда: -68.04, Время: 0:39:09.951869\n",
            "Эпизод 150, Средняя награда: -46.35, Время: 0:41:57.767804\n",
            "Эпизод 160, Средняя награда: -50.95, Время: 0:44:43.922148\n",
            "Эпизод 170, Средняя награда: -56.95, Время: 0:47:24.985939\n",
            "Эпизод 180, Средняя награда: -72.23, Время: 0:50:14.682339\n",
            "Эпизод 190, Средняя награда: -67.37, Время: 0:53:01.430466\n",
            "Эпизод 200, Средняя награда: -73.80, Время: 0:55:35.868037\n",
            "Эпизод 210, Средняя награда: -48.76, Время: 0:58:25.625929\n",
            "Эпизод 220, Средняя награда: -66.47, Время: 1:01:12.671418\n",
            "Эпизод 230, Средняя награда: -57.70, Время: 1:04:02.352348\n",
            "Эпизод 240, Средняя награда: -64.73, Время: 1:06:54.906411\n",
            "Эпизод 250, Средняя награда: -61.90, Время: 1:09:44.615867\n",
            "Эпизод 260, Средняя награда: -70.41, Время: 1:12:37.369841\n",
            "Эпизод 270, Средняя награда: -60.51, Время: 1:15:25.277223\n",
            "Эпизод 280, Средняя награда: -67.74, Время: 1:18:14.460241\n",
            "Эпизод 290, Средняя награда: -68.82, Время: 1:21:11.063527\n",
            "Эпизод 300, Средняя награда: -61.27, Время: 1:24:02.903955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ol2k6wZxARYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}